#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»æµåª’ä½“è§‚ç‚¹èšåˆæ¨¡å—
åŸºäºFinBERTçš„é‡‘èæ–°é—»æƒ…æ„Ÿåˆ†æç³»ç»Ÿ
æ”¯æŒä¸­è‹±æ–‡è´¢ç»åª’ä½“è§‚ç‚¹èšåˆ
"""

import os
import sys
import logging
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import time
import sqlite3
from typing import List, Dict, Optional, Tuple
import feedparser
import re
from collections import Counter
import jieba
import jieba.analyse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from Core.DB.db_utils import get_db_connection

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('media_sentiment.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MediaSentimentAnalyzer:
    """
    åª’ä½“è§‚ç‚¹èšåˆä¸æƒ…æ„Ÿåˆ†æå™¨
    """
    
    def __init__(self):
        self.finbert_available = False
        self.init_finbert()
        self.init_chinese_nlp()
        
        # æ–°é—»æºé…ç½®
        self.news_sources = {
            # è‹±æ–‡åª’ä½“ RSS æº
            'english': {
                'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline?s=^GSPC,^DJI,^IXIC&region=US&lang=en-US',
                'reuters_business': 'https://feeds.reuters.com/reuters/businessNews',
                'bloomberg_markets': 'https://feeds.bloomberg.com/markets/news.rss',
                'marketwatch': 'https://feeds.marketwatch.com/marketwatch/marketpulse/',
                'cnbc_markets': 'https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=15839069'
            },
            # ä¸­æ–‡åª’ä½“ RSS æº
            'chinese': {
                'sina_finance': 'https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=1686&k=&num=50&page=1',
                'eastmoney': 'http://feed.eastmoney.com/rssapi/news?type=cjxw&pageSize=50',
                'caixin': 'https://www.caixin.com/rss/rss_finance.xml',
                'yicai': 'https://www.yicai.com/api/ajax/getlatest?page=1&pagesize=50&action=news',
                'wallstreetcn': 'https://api-prod.wallstreetcn.com/apiv1/content/articles?limit=50&cursor=0'
            }
        }
        
        # æƒ…æ„Ÿè¯å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.sentiment_dict = {
            'positive': ['ä¸Šæ¶¨', 'å¢é•¿', 'åˆ©å¥½', 'ä¹è§‚', 'å¼ºåŠ²', 'çªç ´', 'åˆ›æ–°é«˜', 'bullish', 'positive', 'growth', 'surge', 'rally'],
            'negative': ['ä¸‹è·Œ', 'ä¸‹æ»‘', 'åˆ©ç©º', 'æ‚²è§‚', 'ç–²è½¯', 'è·Œç ´', 'åˆ›æ–°ä½', 'bearish', 'negative', 'decline', 'crash', 'fall']
        }
    
    def init_finbert(self):
        """
        åˆå§‹åŒ–FinBERTæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            import torch
            
            logger.info("æ­£åœ¨åŠ è½½FinBERTæ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
            self.model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
            self.finbert_available = True
            logger.info("FinBERTæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except ImportError:
            logger.warning("transformersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–æƒ…æ„Ÿåˆ†æ")
            self.finbert_available = False
        except Exception as e:
            logger.error(f"FinBERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.finbert_available = False
    
    def init_chinese_nlp(self):
        """
        åˆå§‹åŒ–ä¸­æ–‡NLPå·¥å…·
        """
        try:
            # è®¾ç½®jiebaè¯å…¸
            jieba.set_dictionary(None)  # ä½¿ç”¨é»˜è®¤è¯å…¸
            logger.info("ä¸­æ–‡NLPå·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"ä¸­æ–‡NLPå·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def analyze_sentiment_finbert(self, texts: List[str]) -> List[Dict]:
        """
        ä½¿ç”¨FinBERTè¿›è¡Œæƒ…æ„Ÿåˆ†æ
        """
        if not self.finbert_available:
            return self.analyze_sentiment_simple(texts)
        
        try:
            import torch
            import torch.nn.functional as F
            
            results = []
            
            # æ‰¹é‡å¤„ç†
            batch_size = 16
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # é¢„å¤„ç†æ–‡æœ¬
                inputs = self.tokenizer(batch_texts, 
                                       padding=True, 
                                       truncation=True, 
                                       max_length=512,
                                       return_tensors='pt')
                
                # æ¨ç†
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    predictions = F.softmax(outputs.logits, dim=-1)
                
                # è§£æç»“æœ
                for j, pred in enumerate(predictions):
                    scores = pred.numpy()
                    # FinBERTè¾“å‡º: [negative, neutral, positive]
                    sentiment_score = scores[2] - scores[0]  # positive - negative
                    
                    if sentiment_score > 0.1:
                        sentiment = 'positive'
                    elif sentiment_score < -0.1:
                        sentiment = 'negative'
                    else:
                        sentiment = 'neutral'
                    
                    results.append({
                        'text': batch_texts[j],
                        'sentiment': sentiment,
                        'score': float(sentiment_score),
                        'confidence': float(max(scores)),
                        'scores': {
                            'negative': float(scores[0]),
                            'neutral': float(scores[1]),
                            'positive': float(scores[2])
                        }
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"FinBERTåˆ†æå¤±è´¥: {e}")
            return self.analyze_sentiment_simple(texts)
    
    def analyze_sentiment_simple(self, texts: List[str]) -> List[Dict]:
        """
        ç®€åŒ–ç‰ˆæƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
        """
        results = []
        
        for text in texts:
            text_lower = text.lower()
            
            # è®¡ç®—æ­£è´Ÿé¢è¯æ±‡æ•°é‡
            positive_count = sum(1 for word in self.sentiment_dict['positive'] 
                               if word in text_lower)
            negative_count = sum(1 for word in self.sentiment_dict['negative'] 
                               if word in text_lower)
            
            # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
            if positive_count > negative_count:
                sentiment = 'positive'
                score = (positive_count - negative_count) / max(len(text.split()), 1)
            elif negative_count > positive_count:
                sentiment = 'negative'
                score = -(negative_count - positive_count) / max(len(text.split()), 1)
            else:
                sentiment = 'neutral'
                score = 0.0
            
            results.append({
                'text': text,
                'sentiment': sentiment,
                'score': score,
                'confidence': min(abs(score) * 2, 1.0),
                'method': 'keyword_based'
            })
        
        return results
    
    def fetch_rss_news(self, url: str, source_name: str) -> List[Dict]:
        """
        è·å–RSSæ–°é—»
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            
            articles = []
            for entry in feed.entries[:20]:  # é™åˆ¶æ¯ä¸ªæºæœ€å¤š20æ¡
                article = {
                    'title': entry.get('title', ''),
                    'description': entry.get('description', ''),
                    'link': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': source_name,
                    'language': 'chinese' if source_name in self.news_sources['chinese'] else 'english'
                }
                articles.append(article)
            
            logger.info(f"ä» {source_name} è·å–åˆ° {len(articles)} æ¡æ–°é—»")
            return articles
            
        except Exception as e:
            logger.error(f"è·å– {source_name} æ–°é—»å¤±è´¥: {e}")
            return []
    
    def fetch_all_news(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰æ–°é—»æºçš„æ–°é—»
        """
        all_articles = []
        
        # è·å–è‹±æ–‡æ–°é—»
        for source_name, url in self.news_sources['english'].items():
            articles = self.fetch_rss_news(url, source_name)
            all_articles.extend(articles)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # è·å–ä¸­æ–‡æ–°é—»
        for source_name, url in self.news_sources['chinese'].items():
            articles = self.fetch_rss_news(url, source_name)
            all_articles.extend(articles)
            time.sleep(1)
        
        logger.info(f"æ€»å…±è·å–åˆ° {len(all_articles)} æ¡æ–°é—»")
        return all_articles
    
    def extract_keywords(self, text: str, language: str = 'english') -> List[str]:
        """
        æå–å…³é”®è¯
        """
        if language == 'chinese':
            # ä¸­æ–‡å…³é”®è¯æå–
            keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=False)
            return keywords
        else:
            # è‹±æ–‡å…³é”®è¯æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
            words = re.findall(r'\b[A-Za-z]{3,}\b', text.lower())
            # è¿‡æ»¤å¸¸è§åœç”¨è¯
            stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'man', 'new', 'now', 'old', 'see', 'two', 'way', 'who', 'boy', 'did', 'its', 'let', 'put', 'say', 'she', 'too', 'use'}
            keywords = [word for word in words if word not in stop_words]
            return list(Counter(keywords).most_common(10))
    
    def generate_report(self, articles: List[Dict], sentiment_results: List[Dict]) -> str:
        """
        ç”Ÿæˆåª’ä½“è§‚ç‚¹èšåˆæŠ¥å‘Š
        """
        if not articles or not sentiment_results:
            return "# åª’ä½“è§‚ç‚¹èšåˆæŠ¥å‘Š\n\næš‚æ— æ•°æ®"
        
        # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
        sentiment_counts = Counter([r['sentiment'] for r in sentiment_results])
        total_articles = len(sentiment_results)
        
        positive_pct = (sentiment_counts.get('positive', 0) / total_articles) * 100
        neutral_pct = (sentiment_counts.get('neutral', 0) / total_articles) * 100
        negative_pct = (sentiment_counts.get('negative', 0) / total_articles) * 100
        
        # è®¡ç®—å¹³å‡æƒ…æ„Ÿå¾—åˆ†
        avg_score = np.mean([r.get('score', 0) for r in sentiment_results])
        
        # æå–çƒ­ç‚¹å…³é”®è¯
        all_keywords = []
        for article in articles:
            text = f"{article['title']} {article['description']}"
            keywords = self.extract_keywords(text, article['language'])
            all_keywords.extend(keywords)
        
        top_keywords = Counter(all_keywords).most_common(10)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# ğŸ“Š ä¸»æµåª’ä½“è§‚ç‚¹èšåˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: Bloomberg, Reuters, Yahoo Finance, æ–°æµªè´¢ç», ä¸œæ–¹è´¢å¯Œ, è´¢æ–°ç½‘ç­‰
**åˆ†ææ–¹æ³•**: {'FinBERTæ·±åº¦å­¦ä¹ æ¨¡å‹' if self.finbert_available else 'å…³é”®è¯æƒ…æ„Ÿåˆ†æ'}

## ğŸ“ˆ å¸‚åœºæƒ…ç»ªæ¦‚è§ˆ

- **æ•´ä½“æƒ…ç»ª**: {'ä¹è§‚' if avg_score > 0.1 else 'æ‚²è§‚' if avg_score < -0.1 else 'ä¸­æ€§'} (å¾—åˆ†: {avg_score:.3f})
- **æƒ…æ„Ÿåˆ†å¸ƒ**: 
  - ğŸŸ¢ æ­£é¢: {positive_pct:.1f}% ({sentiment_counts.get('positive', 0)}æ¡)
  - ğŸŸ¡ ä¸­æ€§: {neutral_pct:.1f}% ({sentiment_counts.get('neutral', 0)}æ¡)
  - ğŸ”´ è´Ÿé¢: {negative_pct:.1f}% ({sentiment_counts.get('negative', 0)}æ¡)
- **æ ·æœ¬æ•°é‡**: {total_articles}æ¡æ–°é—»

## ğŸ”¥ çƒ­ç‚¹å…³é”®è¯

"""
        
        for i, (keyword, count) in enumerate(top_keywords[:8], 1):
            report += f"{i}. **{keyword}** ({count}æ¬¡)\n"
        
        report += "\n## ğŸ“° é‡ç‚¹æ–°é—»åˆ†æ\n\n"
        
        # é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ–°é—»
        sorted_results = sorted(sentiment_results, key=lambda x: abs(x.get('score', 0)), reverse=True)
        
        for i, result in enumerate(sorted_results[:6], 1):
            article = next((a for a in articles if f"{a['title']} {a['description']}" == result['text']), None)
            if article:
                sentiment_emoji = {'positive': 'ğŸŸ¢', 'negative': 'ğŸ”´', 'neutral': 'ğŸŸ¡'}[result['sentiment']]
                confidence = result.get('confidence', 0)
                
                report += f"""### {sentiment_emoji} {article['source'].upper()}: "{article['title'][:50]}..."

- **æƒ…æ„Ÿ**: {result['sentiment']} (å¾—åˆ†: {result.get('score', 0):.3f}, ç½®ä¿¡åº¦: {confidence:.2f})
- **æ¥æº**: {article['source']}
- **é“¾æ¥**: [{article['title'][:30]}...]({article['link']})
- **æ‘˜è¦**: {article['description'][:100]}...

"""
        
        report += """## âš ï¸ å…è´£å£°æ˜

æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€åª’ä½“ä¿¡æ¯å’ŒAIæƒ…æ„Ÿåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…ã€‚

---
*æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆ | æ•°æ®æ›´æ–°é¢‘ç‡: æ¯æ—¥*
"""
        
        return report
    
    def save_to_database(self, articles: List[Dict], sentiment_results: List[Dict]):
        """
        ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
        """
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS media_sentiment (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT,
                    description TEXT,
                    source TEXT,
                    language TEXT,
                    link TEXT,
                    published TEXT,
                    sentiment TEXT,
                    sentiment_score REAL,
                    confidence REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # æ’å…¥æ•°æ®
            for article, sentiment in zip(articles, sentiment_results):
                cursor.execute("""
                    INSERT INTO media_sentiment 
                    (title, description, source, language, link, published, 
                     sentiment, sentiment_score, confidence)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    article['title'],
                    article['description'],
                    article['source'],
                    article['language'],
                    article['link'],
                    article['published'],
                    sentiment['sentiment'],
                    sentiment.get('score', 0),
                    sentiment.get('confidence', 0)
                ))
            
            conn.commit()
            conn.close()
            
            logger.info(f"æˆåŠŸä¿å­˜ {len(articles)} æ¡è®°å½•åˆ°æ•°æ®åº“")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")
    
    def run_analysis(self) -> str:
        """
        è¿è¡Œå®Œæ•´çš„åª’ä½“è§‚ç‚¹åˆ†æ
        """
        logger.info("å¼€å§‹åª’ä½“è§‚ç‚¹èšåˆåˆ†æ...")
        
        # 1. è·å–æ–°é—»
        articles = self.fetch_all_news()
        if not articles:
            logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»")
            return "# åª’ä½“è§‚ç‚¹èšåˆæŠ¥å‘Š\n\næš‚æ— æ•°æ®"
        
        # 2. å‡†å¤‡æ–‡æœ¬æ•°æ®
        texts = [f"{article['title']} {article['description']}" for article in articles]
        
        # 3. æƒ…æ„Ÿåˆ†æ
        logger.info("å¼€å§‹æƒ…æ„Ÿåˆ†æ...")
        sentiment_results = self.analyze_sentiment_finbert(texts)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(articles, sentiment_results)
        
        # 5. ä¿å­˜åˆ°æ•°æ®åº“
        self.save_to_database(articles, sentiment_results)
        
        # 6. ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        report_filename = f"media_sentiment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'plot_html', report_filename)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return report

def main():
    """
    ä¸»å‡½æ•°
    """
    analyzer = MediaSentimentAnalyzer()
    report = analyzer.run_analysis()
    print(report)

if __name__ == "__main__":
    main()