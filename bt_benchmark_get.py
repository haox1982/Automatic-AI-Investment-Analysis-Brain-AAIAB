#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»æµåª’ä½“è§‚ç‚¹èšåˆåˆ†æå·¥å…·
åŸºäºFinBERTå’Œå…³é”®è¯åˆ†æï¼Œèšåˆä¸­è‹±æ–‡è´¢ç»åª’ä½“è§‚ç‚¹
é€‚ç”¨äºä¸­é•¿çº¿æŠ•èµ„å†³ç­–æ”¯æŒ
"""

import os
import sys
import argparse
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import json
import time

# æ•°æ®å¤„ç†
import pandas as pd
import numpy as np

# HTTPè¯·æ±‚
import requests
from urllib.parse import quote

# RSSè§£æ
import feedparser

# ä¸­æ–‡åˆ†è¯
import jieba

# ç¯å¢ƒå˜é‡
from dotenv import load_dotenv

# æ•°æ®åº“
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# å°è¯•å¯¼å…¥transformersï¼ˆFinBERTï¼‰
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    FINBERT_AVAILABLE = True
except ImportError:
    FINBERT_AVAILABLE = False
    print("è­¦å‘Š: transformersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„å…³é”®è¯åˆ†ææ–¹æ³•")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('media_sentiment.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MediaSentimentAnalyzer:
    """åª’ä½“æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self):
        self.db_engine = None
        self.finbert_model = None
        self.finbert_tokenizer = None
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        self._init_database()
        
        # åˆå§‹åŒ–FinBERTæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if FINBERT_AVAILABLE:
            self._init_finbert()
        
        # ä¸­è‹±æ–‡åª’ä½“æºé…ç½®
        self.media_sources = {
            'english': {
                'yahoo_finance': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
                'reuters_business': 'https://www.reuters.com/business/finance/rss',
                'marketwatch': 'https://feeds.marketwatch.com/marketwatch/topstories/',
            },
            'chinese': {
                'sina_finance': 'https://feed.sina.com.cn/api/roll/get?pageid=153&lid=1686&k=&num=50&page=1',
                'eastmoney': 'https://api.eastmoney.com/news/list?type=1&pageSize=50&pageIndex=1',
            }
        }
        
        # æƒ…æ„Ÿåˆ†æå…³é”®è¯ï¼ˆä¸­æ–‡ï¼‰
        self.sentiment_keywords = {
            'positive': ['ä¸Šæ¶¨', 'å¢é•¿', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹è§‚', 'çªç ´', 'åˆ›æ–°é«˜', 'å¼ºåŠ¿', 'å›å‡', 'åå¼¹'],
            'negative': ['ä¸‹è·Œ', 'ä¸‹æ»‘', 'åˆ©ç©º', 'çœ‹ç©º', 'æ‚²è§‚', 'è·Œç ´', 'åˆ›æ–°ä½', 'ç–²è½¯', 'å›è½', 'æš´è·Œ'],
            'neutral': ['æŒå¹³', 'éœ‡è¡', 'è§‚æœ›', 'è°¨æ…', 'ç¨³å®š', 'ç»´æŒ', 'é¢„æœŸ', 'å…³æ³¨']
        }
        
        # èµ„äº§ç±»åˆ«å…³é”®è¯
        self.asset_keywords = {
            'è‚¡ç¥¨': ['è‚¡ç¥¨', 'è‚¡å¸‚', 'ä¸Šè¯', 'æ·±è¯', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿', 'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡'],
            'å€ºåˆ¸': ['å€ºåˆ¸', 'å›½å€º', 'ä¼ä¸šå€º', 'å¯è½¬å€º', 'åˆ©ç‡', 'æ”¶ç›Šç‡'],
            'å•†å“': ['é»„é‡‘', 'åŸæ²¹', 'é“œ', 'é“çŸ¿çŸ³', 'å¤§è±†', 'ç‰ç±³', 'å•†å“'],
            'æ±‡ç‡': ['äººæ°‘å¸', 'ç¾å…ƒ', 'æ¬§å…ƒ', 'æ—¥å…ƒ', 'æ±‡ç‡', 'å¤–æ±‡'],
            'æˆ¿åœ°äº§': ['æˆ¿åœ°äº§', 'æˆ¿ä»·', 'åœ°äº§', 'ä½æˆ¿', 'æ¥¼å¸‚']
        }
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            db_url = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
            self.db_engine = create_engine(db_url)
            logger.info("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            self.db_engine = None
    
    def _init_finbert(self):
        """åˆå§‹åŒ–FinBERTæ¨¡å‹"""
        try:
            model_name = "ProsusAI/finbert"
            self.finbert_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.finbert_model = AutoModelForSequenceClassification.from_pretrained(model_name)
            logger.info("FinBERTæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"FinBERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.finbert_model = None
            self.finbert_tokenizer = None
    
    def get_news_from_rss(self, url: str, max_items: int = 20) -> List[Dict]:
        """ä»RSSæºè·å–æ–°é—»"""
        try:
            feed = feedparser.parse(url)
            news_items = []
            
            for entry in feed.entries[:max_items]:
                news_item = {
                    'title': entry.get('title', ''),
                    'summary': entry.get('summary', ''),
                    'link': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'source': url
                }
                news_items.append(news_item)
            
            return news_items
        except Exception as e:
            logger.error(f"RSSè·å–å¤±è´¥ {url}: {e}")
            return []
    
    def get_chinese_news(self, source: str) -> List[Dict]:
        """è·å–ä¸­æ–‡æ–°é—»ï¼ˆæ¨¡æ‹ŸAPIè°ƒç”¨ï¼‰"""
        # è¿™é‡Œæ˜¯ç¤ºä¾‹å®ç°ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“APIè°ƒæ•´
        try:
            # æ¨¡æ‹Ÿæ–°é—»æ•°æ®
            sample_news = [
                {
                    'title': 'Aè‚¡ä¸‰å¤§æŒ‡æ•°é›†ä½“ä¸Šæ¶¨ï¼Œç§‘æŠ€è‚¡è¡¨ç°å¼ºåŠ¿',
                    'summary': 'ä»Šæ—¥Aè‚¡å¸‚åœºè¡¨ç°è‰¯å¥½ï¼Œä¸Šè¯æŒ‡æ•°ä¸Šæ¶¨1.2%ï¼Œæ·±è¯æˆæŒ‡ä¸Šæ¶¨1.5%ï¼Œåˆ›ä¸šæ¿æŒ‡ä¸Šæ¶¨2.1%ã€‚ç§‘æŠ€è‚¡é¢†æ¶¨ï¼Œæ–°èƒ½æºæ¿å—ä¹Ÿæœ‰ä¸é”™è¡¨ç°ã€‚',
                    'link': 'https://finance.sina.com.cn/example1',
                    'published': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'source': source
                },
                {
                    'title': 'å¤®è¡Œç»´æŒåˆ©ç‡ä¸å˜ï¼Œå¸‚åœºé¢„æœŸç¨³å®š',
                    'summary': 'ä¸­å›½äººæ°‘é“¶è¡Œä»Šæ—¥å®£å¸ƒç»´æŒåŸºå‡†åˆ©ç‡ä¸å˜ï¼Œç¬¦åˆå¸‚åœºé¢„æœŸã€‚åˆ†æå¸ˆè®¤ä¸ºè¿™æœ‰åˆ©äºç»´æŠ¤å¸‚åœºç¨³å®šã€‚',
                    'link': 'https://finance.sina.com.cn/example2',
                    'published': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'source': source
                }
            ]
            return sample_news
        except Exception as e:
            logger.error(f"ä¸­æ–‡æ–°é—»è·å–å¤±è´¥ {source}: {e}")
            return []
    
    def analyze_sentiment_finbert(self, text: str) -> Dict[str, float]:
        """ä½¿ç”¨FinBERTè¿›è¡Œæƒ…æ„Ÿåˆ†æ"""
        if not self.finbert_model or not self.finbert_tokenizer:
            return self.analyze_sentiment_keywords(text)
        
        try:
            inputs = self.finbert_tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            
            with torch.no_grad():
                outputs = self.finbert_model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
            # FinBERTè¾“å‡º: [negative, neutral, positive]
            scores = predictions[0].tolist()
            
            return {
                'negative': scores[0],
                'neutral': scores[1],
                'positive': scores[2],
                'method': 'finbert'
            }
        except Exception as e:
            logger.error(f"FinBERTåˆ†æå¤±è´¥: {e}")
            return self.analyze_sentiment_keywords(text)
    
    def analyze_sentiment_keywords(self, text: str) -> Dict[str, float]:
        """åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æï¼ˆä¸­æ–‡é€‚ç”¨ï¼‰"""
        # ä¸­æ–‡åˆ†è¯
        words = list(jieba.cut(text.lower()))
        
        positive_count = sum(1 for word in words if any(keyword in word for keyword in self.sentiment_keywords['positive']))
        negative_count = sum(1 for word in words if any(keyword in word for keyword in self.sentiment_keywords['negative']))
        neutral_count = sum(1 for word in words if any(keyword in word for keyword in self.sentiment_keywords['neutral']))
        
        total_sentiment_words = positive_count + negative_count + neutral_count
        
        if total_sentiment_words == 0:
            return {'positive': 0.33, 'neutral': 0.34, 'negative': 0.33, 'method': 'keywords'}
        
        return {
            'positive': positive_count / total_sentiment_words,
            'neutral': neutral_count / total_sentiment_words,
            'negative': negative_count / total_sentiment_words,
            'method': 'keywords'
        }
    
    def extract_asset_categories(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­æ¶‰åŠçš„èµ„äº§ç±»åˆ«"""
        categories = []
        text_lower = text.lower()
        
        for category, keywords in self.asset_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                categories.append(category)
        
        return categories if categories else ['ç»¼åˆ']
    
    def collect_all_news(self, days: int = 7) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰åª’ä½“æºçš„æ–°é—»"""
        all_news = []
        
        # æ”¶é›†è‹±æ–‡æ–°é—»
        for source_name, url in self.media_sources['english'].items():
            logger.info(f"æ­£åœ¨è·å– {source_name} æ–°é—»...")
            news_items = self.get_news_from_rss(url)
            for item in news_items:
                item['language'] = 'english'
                item['source_name'] = source_name
            all_news.extend(news_items)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # æ”¶é›†ä¸­æ–‡æ–°é—»
        for source_name in self.media_sources['chinese'].keys():
            logger.info(f"æ­£åœ¨è·å– {source_name} æ–°é—»...")
            news_items = self.get_chinese_news(source_name)
            for item in news_items:
                item['language'] = 'chinese'
                item['source_name'] = source_name
            all_news.extend(news_items)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        logger.info(f"æ€»å…±æ”¶é›†åˆ° {len(all_news)} æ¡æ–°é—»")
        return all_news
    
    def analyze_news_batch(self, news_list: List[Dict]) -> List[Dict]:
        """æ‰¹é‡åˆ†ææ–°é—»æƒ…æ„Ÿ"""
        analyzed_news = []
        
        for news in news_list:
            try:
                # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
                text = f"{news.get('title', '')} {news.get('summary', '')}"
                
                # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†ææ–¹æ³•
                if news.get('language') == 'english' and FINBERT_AVAILABLE:
                    sentiment = self.analyze_sentiment_finbert(text)
                else:
                    sentiment = self.analyze_sentiment_keywords(text)
                
                # æå–èµ„äº§ç±»åˆ«
                asset_categories = self.extract_asset_categories(text)
                
                # æ·»åŠ åˆ†æç»“æœ
                news['sentiment'] = sentiment
                news['asset_categories'] = asset_categories
                news['analyzed_at'] = datetime.now().isoformat()
                
                analyzed_news.append(news)
                
            except Exception as e:
                logger.error(f"æ–°é—»åˆ†æå¤±è´¥: {e}")
                continue
        
        return analyzed_news
    
    def generate_aggregated_report(self, analyzed_news: List[Dict]) -> Dict:
        """ç”ŸæˆèšåˆæŠ¥å‘Š"""
        if not analyzed_news:
            return {'error': 'æ²¡æœ‰å¯åˆ†æçš„æ–°é—»æ•°æ®'}
        
        # æŒ‰èµ„äº§ç±»åˆ«èšåˆ
        asset_sentiment = {}
        for news in analyzed_news:
            for asset in news.get('asset_categories', ['ç»¼åˆ']):
                if asset not in asset_sentiment:
                    asset_sentiment[asset] = {'positive': [], 'neutral': [], 'negative': []}
                
                sentiment = news.get('sentiment', {})
                asset_sentiment[asset]['positive'].append(sentiment.get('positive', 0))
                asset_sentiment[asset]['neutral'].append(sentiment.get('neutral', 0))
                asset_sentiment[asset]['negative'].append(sentiment.get('negative', 0))
        
        # è®¡ç®—å¹³å‡æƒ…æ„Ÿ
        aggregated_sentiment = {}
        for asset, sentiments in asset_sentiment.items():
            aggregated_sentiment[asset] = {
                'positive': np.mean(sentiments['positive']) if sentiments['positive'] else 0,
                'neutral': np.mean(sentiments['neutral']) if sentiments['neutral'] else 0,
                'negative': np.mean(sentiments['negative']) if sentiments['negative'] else 0,
                'news_count': len(sentiments['positive'])
            }
        
        # æ•´ä½“å¸‚åœºæƒ…æ„Ÿ
        all_positive = [news['sentiment']['positive'] for news in analyzed_news if 'sentiment' in news]
        all_neutral = [news['sentiment']['neutral'] for news in analyzed_news if 'sentiment' in news]
        all_negative = [news['sentiment']['negative'] for news in analyzed_news if 'sentiment' in news]
        
        overall_sentiment = {
            'positive': np.mean(all_positive) if all_positive else 0,
            'neutral': np.mean(all_neutral) if all_neutral else 0,
            'negative': np.mean(all_negative) if all_negative else 0
        }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'generated_at': datetime.now().isoformat(),
            'analysis_period': '7å¤©',
            'total_news_analyzed': len(analyzed_news),
            'overall_market_sentiment': overall_sentiment,
            'asset_sentiment_breakdown': aggregated_sentiment,
            'key_insights': self._generate_insights(overall_sentiment, aggregated_sentiment),
            'risk_alerts': self._generate_risk_alerts(aggregated_sentiment)
        }
        
        return report
    
    def _generate_insights(self, overall: Dict, by_asset: Dict) -> List[str]:
        """ç”Ÿæˆå…³é”®æ´å¯Ÿ"""
        insights = []
        
        # æ•´ä½“å¸‚åœºæƒ…æ„Ÿåˆ¤æ–­
        if overall['positive'] > 0.4:
            insights.append("å¸‚åœºæ•´ä½“æƒ…æ„Ÿåå‘ä¹è§‚ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒè¾ƒå¼º")
        elif overall['negative'] > 0.4:
            insights.append("å¸‚åœºæ•´ä½“æƒ…æ„Ÿåå‘æ‚²è§‚ï¼Œéœ€è¦å…³æ³¨é£é™©")
        else:
            insights.append("å¸‚åœºæƒ…æ„Ÿç›¸å¯¹ä¸­æ€§ï¼Œå¤„äºè§‚æœ›çŠ¶æ€")
        
        # èµ„äº§ç±»åˆ«åˆ†æ
        for asset, sentiment in by_asset.items():
            if sentiment['positive'] > 0.5:
                insights.append(f"{asset}æ¿å—æƒ…æ„Ÿç§¯æï¼Œå¯èƒ½å­˜åœ¨æŠ•èµ„æœºä¼š")
            elif sentiment['negative'] > 0.5:
                insights.append(f"{asset}æ¿å—æƒ…æ„Ÿæ¶ˆæï¼Œå»ºè®®è°¨æ…æ“ä½œ")
        
        return insights
    
    def _generate_risk_alerts(self, by_asset: Dict) -> List[str]:
        """ç”Ÿæˆé£é™©é¢„è­¦"""
        alerts = []
        
        for asset, sentiment in by_asset.items():
            if sentiment['negative'] > 0.6:
                alerts.append(f"âš ï¸ {asset}æ¿å—è´Ÿé¢æƒ…æ„Ÿè¿‡é«˜({sentiment['negative']:.2%})ï¼Œå»ºè®®å¯†åˆ‡å…³æ³¨")
            
            if sentiment['news_count'] < 3:
                alerts.append(f"â„¹ï¸ {asset}æ¿å—æ–°é—»æ•°é‡è¾ƒå°‘({sentiment['news_count']}æ¡)ï¼Œåˆ†æç»“æœå¯èƒ½ä¸å¤Ÿå…¨é¢")
        
        return alerts
    
    def save_to_database(self, report: Dict, analyzed_news: List[Dict]):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“"""
        if not self.db_engine:
            logger.error("æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ï¼Œæ— æ³•ä¿å­˜æ•°æ®")
            return False
        
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®åº“è¡¨ç»“æ„è°ƒæ•´
            # ç¤ºä¾‹ï¼šä¿å­˜åˆ°media_sentimentè¡¨
            with self.db_engine.connect() as conn:
                # ä¿å­˜èšåˆæŠ¥å‘Š
                insert_report_sql = text("""
                    INSERT INTO media_sentiment_reports 
                    (generated_at, report_data, news_count) 
                    VALUES (:generated_at, :report_data, :news_count)
                """)
                
                conn.execute(insert_report_sql, {
                    'generated_at': datetime.now(),
                    'report_data': json.dumps(report, ensure_ascii=False),
                    'news_count': len(analyzed_news)
                })
                
                conn.commit()
                logger.info("åˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")
                return True
                
        except SQLAlchemyError as e:
            logger.error(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def run_analysis(self, days: int = 7, target_assets: Optional[List[str]] = None) -> Dict:
        """è¿è¡Œå®Œæ•´çš„åª’ä½“è§‚ç‚¹åˆ†æ"""
        logger.info(f"å¼€å§‹åª’ä½“è§‚ç‚¹èšåˆåˆ†æï¼Œåˆ†æå‘¨æœŸ: {days}å¤©")
        
        # 1. æ”¶é›†æ–°é—»
        news_list = self.collect_all_news(days)
        if not news_list:
            return {'error': 'æœªèƒ½è·å–åˆ°æ–°é—»æ•°æ®'}
        
        # 2. åˆ†ææƒ…æ„Ÿ
        analyzed_news = self.analyze_news_batch(news_list)
        
        # 3. è¿‡æ»¤ç›®æ ‡èµ„äº§ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if target_assets:
            filtered_news = []
            for news in analyzed_news:
                if any(asset in news.get('asset_categories', []) for asset in target_assets):
                    filtered_news.append(news)
            analyzed_news = filtered_news
        
        # 4. ç”ŸæˆèšåˆæŠ¥å‘Š
        report = self.generate_aggregated_report(analyzed_news)
        
        # 5. ä¿å­˜åˆ°æ•°æ®åº“
        self.save_to_database(report, analyzed_news)
        
        # 6. è¾“å‡ºæŠ¥å‘Š
        self._print_report(report)
        
        return report
    
    def _print_report(self, report: Dict):
        """æ‰“å°åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š ä¸»æµåª’ä½“è§‚ç‚¹èšåˆåˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if 'error' in report:
            print(f"âŒ é”™è¯¯: {report['error']}")
            return
        
        print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {report['generated_at']}")
        print(f"ğŸ“° åˆ†ææ–°é—»æ•°é‡: {report['total_news_analyzed']}æ¡")
        print(f"â±ï¸ åˆ†æå‘¨æœŸ: {report['analysis_period']}")
        
        # æ•´ä½“å¸‚åœºæƒ…æ„Ÿ
        overall = report['overall_market_sentiment']
        print(f"\nğŸŒ æ•´ä½“å¸‚åœºæƒ…æ„Ÿ:")
        print(f"   ç§¯æ: {overall['positive']:.2%}")
        print(f"   ä¸­æ€§: {overall['neutral']:.2%}")
        print(f"   æ¶ˆæ: {overall['negative']:.2%}")
        
        # å„èµ„äº§ç±»åˆ«æƒ…æ„Ÿ
        print(f"\nğŸ“ˆ å„èµ„äº§ç±»åˆ«æƒ…æ„Ÿåˆ†æ:")
        for asset, sentiment in report['asset_sentiment_breakdown'].items():
            print(f"   {asset}:")
            print(f"     ç§¯æ: {sentiment['positive']:.2%} | ä¸­æ€§: {sentiment['neutral']:.2%} | æ¶ˆæ: {sentiment['negative']:.2%}")
            print(f"     æ–°é—»æ•°é‡: {sentiment['news_count']}æ¡")
        
        # å…³é”®æ´å¯Ÿ
        if report.get('key_insights'):
            print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
            for insight in report['key_insights']:
                print(f"   â€¢ {insight}")
        
        # é£é™©é¢„è­¦
        if report.get('risk_alerts'):
            print(f"\nâš ï¸ é£é™©é¢„è­¦:")
            for alert in report['risk_alerts']:
                print(f"   {alert}")
        
        print("\n" + "="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¸»æµåª’ä½“è§‚ç‚¹èšåˆåˆ†æå·¥å…·')
    parser.add_argument('--days', type=int, default=7, help='åˆ†æå¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰')
    parser.add_argument('--assets', type=str, help='ç›®æ ‡èµ„äº§ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šè‚¡ç¥¨,å€ºåˆ¸ï¼‰')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    
    args = parser.parse_args()
    
    # è§£æç›®æ ‡èµ„äº§
    target_assets = None
    if args.assets:
        target_assets = [asset.strip() for asset in args.assets.split(',')]
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = MediaSentimentAnalyzer()
    report = analyzer.run_analysis(days=args.days, target_assets=target_assets)
    
    # ä¿å­˜è¾“å‡ºæ–‡ä»¶
    if args.output and 'error' not in report:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    main()